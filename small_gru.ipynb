{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Import the library to mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","FOLDERNAME = 'DL project/fma'\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jmXWCFGGbrTC","executionInfo":{"status":"ok","timestamp":1733258732817,"user_tz":480,"elapsed":959,"user":{"displayName":"Frank Wu","userId":"09777963439135390486"}},"outputId":"d02aaf91-e630-4f1c-a49f-7e95328a4dc7"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/.shortcut-targets-by-id/1Gzbs2MQ8ttHuTOpxDaZoGkbdHOw1ukQO/DL project/fma\n"]}]},{"cell_type":"code","source":["import math\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","NUM_CLASSES = 8\n","\n","kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal')\n","\n","def conv3x3(x, out_planes, stride=1, name=None):\n","    x = layers.ZeroPadding1D(padding=1, name=f'{name}_pad')(x)\n","    return layers.Conv1D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=name)(x)\n","\n","def basic_block(x, planes, stride=1, downsample=None, name=None):\n","    identity = x\n","\n","    out = conv3x3(x, planes, stride=stride, name=f'{name}.conv1')\n","    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n","    out = layers.ReLU(name=f'{name}.relu1')(out)\n","\n","    out = conv3x3(out, planes, name=f'{name}.conv2')\n","    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n","\n","    if downsample is not None:\n","        for layer in downsample:\n","            identity = layer(identity)\n","\n","    out = layers.Add(name=f'{name}.add')([identity, out])\n","    out = layers.ReLU(name=f'{name}.relu2')(out)\n","\n","    return out\n","\n","def make_layer(x, planes, blocks, stride=1, name=None):\n","    downsample = None\n","    inplanes = x.shape[2]\n","    if stride != 1 or inplanes != planes:\n","        downsample = [\n","            layers.Conv1D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=f'{name}.0.downsample.0'),\n","            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n","        ]\n","\n","    x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n","    for i in range(1, blocks):\n","        x = basic_block(x, planes, name=f'{name}.{i}')\n","\n","    return x\n","\n","def resnet(x, blocks_per_layer, rnn_n_layers, rnn_type, bidirectional, num_classes=1000):\n","    x = layers.Reshape((x.shape[-1], 1))(x)\n","    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn0')(x)\n","    x = layers.ZeroPadding1D(padding=3, name='conv1_pad')(x)\n","    x = layers.Conv1D(filters=64, kernel_size=7, strides=2, use_bias=False, kernel_initializer=kaiming_normal, name='conv1')(x)\n","    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n","    x = layers.ReLU(name='relu1')(x)\n","    x = layers.ZeroPadding1D(padding=1, name='maxpool_pad')(x)\n","    x = layers.MaxPool1D(pool_size=3, strides=2, name='maxpool')(x)\n","\n","    x = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n","    # x = make_layer(x, 64, blocks_per_layer[0], stride=3, name='layer11')\n","    x = make_layer(x, 128, blocks_per_layer[1], stride=3, name='layer2')\n","    x = make_layer(x, 256, blocks_per_layer[2], stride=3, name='layer3')\n","    x = make_layer(x, 512, blocks_per_layer[3], stride=3, name='layer4')\n","\n","    x = layers.Conv1D(filters=16, kernel_size=1, strides=1, use_bias=False, kernel_initializer=kaiming_normal, name='convdown')(x)\n","\n","\n","\n","    print(x.shape)\n","    for _ in range(rnn_n_layers):\n","        if rnn_type == 'gru':\n","            rnn_layer = layers.GRU(16, return_sequences=True)\n","        elif rnn_type == 'lstm':\n","            rnn_layer = layers.LSTM(16, return_sequences=True)\n","        elif rnn_type == 'simple':\n","            rnn_layer = layers.SimpleRNN(16, return_sequences=True)\n","        else:\n","            raise ValueError(\"rnn_type must be 'gru', 'lstm', or 'simple'\")\n","    if bidirectional:\n","        rnn_layer = layers.Bidirectional(rnn_layer)\n","\n","    x = rnn_layer(x)\n","    x = layers.GlobalAveragePooling1D(name='avgpool')(x)\n","    initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n","    x = layers.Dense(units=num_classes, kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n","    x = layers.Activation('softmax')(x)\n","\n","\n","\n","    return x\n","\n","def resnet18(x, **kwargs):\n","    return resnet(x, [2, 2, 2, 2], rnn_n_layers=1,\n","                  rnn_type = 'simple', bidirectional= False, **kwargs)\n","\n","inputs = keras.Input(shape=(10000,))\n","# inputs = keras.Input(shape=(59953,))\n","outputs = resnet18(inputs, num_classes=NUM_CLASSES)\n","model = keras.Model(inputs, outputs)\n","\n"],"metadata":{"id":"Fc3y1Wp7gq8P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733258734846,"user_tz":480,"elapsed":303,"user":{"displayName":"Frank Wu","userId":"09777963439135390486"}},"outputId":"8181bc7a-7177-42d5-9cf4-53a5727a9e61"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 93, 16)\n"]}]},{"cell_type":"code","source":["import math\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","NUM_CLASSES = 8\n","\n","kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal')\n","\n","def conv3x3(x, out_planes, stride=1, name=None):\n","    x = layers.ZeroPadding1D(padding=1, name=f'{name}_pad')(x)\n","    return layers.Conv1D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=name)(x)\n","\n","def basic_block(x, planes, stride=1, downsample=None, name=None):\n","    identity = x\n","\n","    out = conv3x3(x, planes, stride=stride, name=f'{name}.conv1')\n","    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n","    out = layers.ReLU(name=f'{name}.relu1')(out)\n","\n","    out = conv3x3(out, planes, name=f'{name}.conv2')\n","    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n","\n","    if downsample is not None:\n","        for layer in downsample:\n","            identity = layer(identity)\n","\n","    out = layers.Add(name=f'{name}.add')([identity, out])\n","    out = layers.ReLU(name=f'{name}.relu2')(out)\n","\n","    return out\n","\n","def make_layer(x, planes, blocks, stride=1, name=None):\n","    downsample = None\n","    inplanes = x.shape[2]\n","    if stride != 1 or inplanes != planes:\n","        downsample = [\n","            layers.Conv1D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=f'{name}.0.downsample.0'),\n","            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n","        ]\n","\n","    x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n","    for i in range(1, blocks):\n","        x = basic_block(x, planes, name=f'{name}.{i}')\n","\n","    return x\n","\n","def resnet(x, blocks_per_layer, num_classes=1000):\n","    x = layers.Reshape((x.shape[-1], 1))(x)\n","    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn0')(x)\n","    # x = layers.ZeroPadding1D(padding=3, name='conv1_pad')(x)\n","    x = layers.Conv1D(filters=64, kernel_size=7, strides=2, use_bias=False, kernel_initializer=kaiming_normal, name='conv1')(x)\n","    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n","    x = layers.ReLU(name='relu1')(x)\n","    # x = layers.ZeroPadding1D(padding=1, name='maxpool_pad')(x)\n","    x = layers.MaxPool1D(pool_size=3, strides=2, name='maxpool')(x)\n","\n","    x = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n","    x = make_layer(x, 128, blocks_per_layer[1], stride=2, name='layer2')\n","    x = make_layer(x, 256, blocks_per_layer[2], stride=2, name='layer3')\n","    x = make_layer(x, 512, blocks_per_layer[3], stride=2, name='layer4')\n","    x = layers.Conv1D(filters=16, kernel_size=1, strides=1, use_bias=False, kernel_initializer=kaiming_normal, name='convdown')(x)\n","\n","\n","    rnn_layer = layers.SimpleRNN(16, return_sequences=True)\n","    x = rnn_layer(x)\n","\n","    x = layers.GlobalAveragePooling1D(name='avgpool')(x)\n","    initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n","    x = layers.Dense(units=num_classes, kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n","    x = layers.Activation('softmax')(x)\n","\n","    return x\n","\n","def resnet18(x, **kwargs):\n","    return resnet(x, [2, 2, 2, 2], **kwargs)\n","\n","inputs = keras.Input(shape=(10000,))\n","# inputs = keras.Input(shape=(59953,))\n","outputs = resnet18(inputs, num_classes=NUM_CLASSES)\n","model = keras.Model(inputs, outputs)\n","\n"],"metadata":{"id":"F-5FajKHH35o","executionInfo":{"status":"ok","timestamp":1733259052154,"user_tz":480,"elapsed":327,"user":{"displayName":"Frank Wu","userId":"09777963439135390486"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g02vMCcfbKBD","executionInfo":{"status":"ok","timestamp":1733259056256,"user_tz":480,"elapsed":737,"user":{"displayName":"Frank Wu","userId":"09777963439135390486"}},"outputId":"cef473fb-925c-43cd-b7b8-45d6c336b4a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["6386\n","6386\n","800\n","800\n","Spectrogram shape: (128, 10000)\n","Label: (128, 8)\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","\n","dataset_dir = \"data/raw_small/\"\n","\n","# Function to load a single .npy file and assign its label\n","def load_npy_file(file_path, label):\n","    # Read the file and decode its path\n","    npy = tf.numpy_function(lambda path: np.load(path).astype(np.float32), [file_path], tf.float32)\n","    npy.set_shape([10000,])  # Set shape (update according to your spectrogram's dimensions)\n","\n","    return npy, label\n","\n","\n","# Function to create a dataset for all files\n","def create_dataset(dataset_dir):\n","    # List all files and infer labels from folder names\n","    all_files = []\n","    all_labels = []\n","    for class_name in sorted(os.listdir(dataset_dir)):\n","        class_path = os.path.join(dataset_dir, class_name)\n","        if not os.path.isdir(class_path):\n","            continue\n","        label = ([int(digit) for digit in class_name])  # Convert folder name to integer label\n","        files = [os.path.join(class_path, f) for f in os.listdir(class_path) if f.endswith(\".npy\")]\n","        all_files.extend(files)\n","        all_labels.extend([label] * len(files))\n","\n","    print(len(all_labels))\n","    print(len(all_files))\n","\n","\n","\n","    # Create a dataset from the files and labels\n","    file_paths = tf.constant(all_files)\n","    labels = tf.constant(all_labels, dtype=tf.int32)\n","    num_classes = 8  # Determine the number of classes\n","    # labels = tf.one_hot(labels, depth=num_classes)  # Apply one-hot encoding\n","    # print(labels)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n","    dataset = dataset.shuffle(len(all_files))  # Shuffle dataset\n","    dataset = dataset.map(load_npy_file, num_parallel_calls=tf.data.AUTOTUNE)  # Load files\n","    batch_size = 128\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return dataset\n","\n","\n","# Create the train dataset\n","train_dataset = create_dataset(dataset_dir+\"train\")\n","val_dataset = create_dataset(dataset_dir+\"val\")\n","\n","\n","# Inspect a sample\n","for spectrogram, label in train_dataset.take(1):\n","    print(\"Spectrogram shape:\", spectrogram.shape)\n","    print(\"Label:\", label.shape)\n","    num_classes = label.shape[1]\n"]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","new_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n","model.compile(\n","    optimizer=new_optimizer ,\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","checkpoint = ModelCheckpoint(\n","    filepath='best_small_crnn_model.keras',  # Filepath to save the model\n","    monitor='val_accuracy',       # Metric to monitor (e.g., 'val_loss', 'val_accuracy')\n","    save_best_only=True,      # Save only the best model\n","    save_weights_only=False,  # Save the entire model (not just weights)\n","    mode='min',               # Save when the monitored metric decreases ('min') or increases ('max')\n","    verbose=1                 # Print a message when saving\n",")\n","\n","\n","\n","\n","history = model.fit(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=40,\n","    verbose=1,\n","    callbacks=[checkpoint]\n","    # callbacks=[PrintEveryFewBatchesCallback(interval=1), checkpoint]\n",")\n","print('///////////////////////////////////')\n","print(history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TctUxBoBj93t","outputId":"e1d8e24a-41aa-49a1-9bdd-fb76ea9278f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n"]}]}]}