{"cells":[{"cell_type":"markdown","metadata":{"id":"LGR9UkkSzOvc"},"source":["# [FMA: A Dataset For Music Analysis](https://github.com/mdeff/fma)\n","\n","MichaÃ«l Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson, EPFL LTS2.\n","\n","## Baselines\n","\n","* This notebook evaluates standard classifiers from scikit-learn on the provided features.\n","* Moreover, it evaluates Deep Learning models on both audio and spectrograms."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwfhIi3kzOve","outputId":"87e2721c-e26b-4515-dc76-5c967a5add90","executionInfo":{"status":"ok","timestamp":1733295489711,"user_tz":480,"elapsed":31720,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"]}],"source":["!pip install python-dotenv\n","!pip install --upgrade librosa\n","\n","import time\n","import os\n","\n","import IPython.display as ipd\n","from tqdm import tqdm_notebook\n","import numpy as np\n","import pandas as pd\n","import keras\n","from keras.layers import Activation, Dense, Conv1D, Conv2D, MaxPooling1D, Flatten, Reshape\n","\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, LabelBinarizer, StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC, LinearSVC\n","#from sklearn.gaussian_process import GaussianProcessClassifier\n","#from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Import the library to mount Google Drive\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42362,"status":"ok","timestamp":1733295544113,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"},"user_tz":480},"id":"P00zrAQOILkZ","outputId":"00867518-dbf2-462c-8a3d-40963958fbd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1Gzbs2MQ8ttHuTOpxDaZoGkbdHOw1ukQO/DL project/fma\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","FOLDERNAME = 'DL project/fma'\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/\n","\n","import utils"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37113,"status":"ok","timestamp":1733295582796,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"},"user_tz":480},"id":"yn4-nC1hzOvf","outputId":"a5e01f12-574b-4efe-fe89-f94f683a22dc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((106574, 52), (106574, 518), (13129, 249))"]},"metadata":{},"execution_count":3}],"source":["# AUDIO_DIR = os.environ.get('AUDIO_DIR')\n","# AUDIO_DIR = os.environ.get('AUDIO_DIR')\n","SPLIT_SIZE = 'small'\n","NUM_CLASSES = 0\n","if SPLIT_SIZE == 'small':\n","    NUM_CLASSES = 8\n","else:\n","    NUM_CLASSES = 16\n","\n","AUDIO_DIR = 'data/raw_' + SPLIT_SIZE\n","\n","tracks = utils.load('data/fma_metadata/tracks.csv')\n","features = utils.load('data/fma_metadata/features.csv')\n","echonest = utils.load('data/fma_metadata/echonest.csv')\n","\n","np.testing.assert_array_equal(features.index, tracks.index)\n","assert echonest.index.isin(tracks.index).all()\n","\n","tracks.shape, features.shape, echonest.shape"]},{"cell_type":"markdown","metadata":{"id":"BCFDs5-AzOvf"},"source":["## Subset"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1733295583030,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"},"user_tz":480},"id":"YEN0CHKazOvf","outputId":"11559c54-5bdb-47f3-9748-2bbcb0173790"},"outputs":[{"output_type":"stream","name":"stdout","text":["Not enough Echonest features: (13129, 767)\n"]},{"output_type":"execute_result","data":{"text/plain":["((8000, 52), (8000, 518))"]},"metadata":{},"execution_count":4}],"source":["subset = tracks.index[tracks['set', 'subset'] <= 'small']\n","\n","assert subset.isin(tracks.index).all()\n","assert subset.isin(features.index).all()\n","\n","features_all = features.join(echonest, how='inner').sort_index(axis=1)\n","print('Not enough Echonest features: {}'.format(features_all.shape))\n","\n","tracks = tracks.loc[subset]\n","features_all = features.loc[subset]\n","\n","tracks.shape, features_all.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1733295583031,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"},"user_tz":480},"id":"oNjKSU5czOvg","outputId":"af1310d0-a08a-4d92-eb49-b315fa8e7f6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["6400 training examples, 800 validation examples, 800 testing examples\n","Top genres (8): ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International', 'Pop', 'Rock']\n","All genres (114): [1, 2, 6, 10, 12, 15, 16, 17, 18, 21, 22, 25, 26, 27, 30, 31, 32, 33, 36, 38, 41, 42, 45, 46, 47, 49, 53, 58, 64, 66, 70, 71, 76, 77, 79, 81, 83, 85, 86, 88, 89, 90, 92, 94, 98, 100, 101, 102, 103, 107, 109, 111, 113, 117, 118, 125, 130, 167, 171, 172, 174, 177, 180, 181, 182, 183, 184, 185, 186, 214, 224, 232, 236, 240, 247, 250, 267, 286, 296, 297, 314, 337, 359, 360, 361, 362, 400, 401, 404, 439, 440, 456, 468, 491, 495, 502, 504, 514, 524, 538, 539, 542, 580, 602, 619, 695, 741, 763, 808, 811, 1032, 1060, 1193, 1235]\n"]}],"source":["train = tracks.index[tracks['set', 'split'] == 'training']\n","val = tracks.index[tracks['set', 'split'] == 'validation']\n","test = tracks.index[tracks['set', 'split'] == 'test']\n","\n","print('{} training examples, {} validation examples, {} testing examples'.format(*map(len, [train, val, test])))\n","\n","genres = list(LabelEncoder().fit(tracks['track', 'genre_top']).classes_)\n","#genres = list(tracks['track', 'genre_top'].unique())\n","print('Top genres ({}): {}'.format(len(genres), genres))\n","genres = list(MultiLabelBinarizer().fit(tracks['track', 'genres_all']).classes_)\n","print('All genres ({}): {}'.format(len(genres), genres))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"xjqIZi8zzOvi","executionInfo":{"status":"ok","timestamp":1733295583248,"user_tz":480,"elapsed":220,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"outputs":[],"source":["tracks['track', 'genre_top'] = tracks['track', 'genre_top'].astype(str)\n","labels_onehot = LabelBinarizer().fit_transform(tracks['track', 'genre_top'])\n","labels_onehot = pd.DataFrame(labels_onehot, index=tracks.index)"]},{"cell_type":"markdown","metadata":{"id":"vUUmvRT9_S9Q"},"source":["## organized in genres\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPhlDRVgNMw3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"18a410d5-421f-40a1-c7e9-6da345fc0a7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current working directory: /content/drive/.shortcut-targets-by-id/1Gzbs2MQ8ttHuTOpxDaZoGkbdHOw1ukQO/DL project/fma\n","(800,)\n","Processing train split...\n"]}],"source":["import os\n","import shutil\n","import pandas as pd\n","from sklearn.preprocessing import LabelBinarizer\n","import numpy as np\n","\n","def create_organized_dataset(tracks_df, source_dir, target_dir):\n","    \"\"\"\n","    Reorganize FMA dataset based on top 8 genres and train/val/test splits.\n","\n","    Args:\n","        tracks_df: DataFrame containing track metadata\n","        source_dir: Path to original FMA dataset (fma_small)\n","        target_dir: Path where reorganized dataset will be stored\n","    \"\"\"\n","    # Create main directories for splits\n","    splits = ['train', 'val', 'test']\n","    for split in splits:\n","        os.makedirs(os.path.join(target_dir, split), exist_ok=True)\n","\n","    # Get indices for each split\n","    split_indices = {\n","        'train': tracks_df.index[tracks_df['set', 'split'] == 'training'],\n","        'val': tracks_df.index[tracks_df['set', 'split'] == 'validation'],\n","        'test': tracks_df.index[tracks_df['set', 'split'] == 'test']\n","    }\n","    print(split_indices['val'].shape)\n","    # Create genre folders within each split\n","    genres = tracks_df['track', 'genre_top'].unique()\n","    for split in splits:\n","        for genre in genres:\n","            os.makedirs(os.path.join(target_dir, split, genre), exist_ok=True)\n","\n","    # Function to get source file path\n","    def get_source_path(track_id):\n","        \"\"\"Convert track ID to source file path.\"\"\"\n","        tid_str = '{:06d}'.format(track_id)\n","        return os.path.join(source_dir, tid_str[:3], tid_str + '.mp3')\n","\n","    # Copy files to new structure\n","    for split_name, indices in split_indices.items():\n","        print(f\"Processing {split_name} split...\")\n","        for idx in indices:\n","            try:\n","                # Get genre and create source/target paths\n","                genre = tracks_df.loc[idx, ('track', 'genre_top')]\n","                source_path = get_source_path(idx)\n","                target_path = os.path.join(target_dir, split_name, genre, f\"{idx:06d}.mp3\")\n","\n","                # Copy file if it exists\n","                # print(source_path)\n","                if os.path.exists(source_path):\n","                  shutil.copy2(source_path, target_path)\n","                # else:\n","                    # print(f\"Warning: Source file not found for track {idx}\")\n","\n","            except Exception as e:\n","                print(f\"Error processing track {idx}: {str(e)}\")\n","\n","    # Create metadata file with one-hot encoded labels\n","    label_binarizer = LabelBinarizer()\n","    labels_onehot = label_binarizer.fit_transform(tracks_df['track', 'genre_top'])\n","\n","    # Create DataFrame with one-hot labels\n","    labels_df = pd.DataFrame(\n","        labels_onehot,\n","        columns=label_binarizer.classes_,\n","        index=tracks_df.index\n","    )\n","\n","    # Add split information\n","    labels_df['split'] = tracks_df['set', 'split']\n","\n","    # Save metadata\n","    labels_df.to_csv(os.path.join(target_dir, 'metadata.csv'))\n","\n","    # Print summary\n","    print(\"\\nDataset reorganization complete!\")\n","    print(\"\\nStructure:\")\n","    for split in splits:\n","        print(f\"\\n{split}:\")\n","        for genre in genres:\n","            genre_path = os.path.join(target_dir, split, genre)\n","            n_files = len(os.listdir(genre_path))\n","            print(f\"  {genre}: {n_files} files\")\n","\n","# Usage example:\n","import os\n","print(\"Current working directory:\", os.getcwd())\n","source_directory = \"data/fma_small\"\n","target_directory = \"data/organized_small\"\n","create_organized_dataset(tracks, source_directory, target_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXSQ7k1DXVSu"},"outputs":[],"source":["splits = ['train', 'val', 'test']\n","target_dir = \"data/organized_small\"\n","for split in splits:\n","        print(f\"\\n{split}:\")\n","        for genre in os.listdir(os.path.join(target_dir, split)):\n","          print(genre)\n","          genre_path = os.path.join(target_dir, split, genre)\n","          n_files = len(os.listdir(genre_path))\n","          print(f\"  {genre}: {n_files} files\")"]},{"cell_type":"markdown","metadata":{"id":"IHoua-SWzOvj"},"source":["## 3 Deep learning on extracted audio features\n","\n","Look at:\n","* Pre-processing in Keras: https://github.com/keunwoochoi/kapre\n","* Convolutional Recurrent Neural Networks for Music Classification: https://github.com/keunwoochoi/icassp_2017\n","* Music Auto-Tagger: https://github.com/keunwoochoi/music-auto_tagging-keras\n","* Pre-processor: https://github.com/bmcfee/pumpp"]},{"cell_type":"markdown","metadata":{"id":"woaU9IbrzOvj"},"source":["### 3.1 ConvNet on MFCC\n","\n","* Architecture: [Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network](http://www.iaeng.org/publication/IMECS2010/IMECS2010_pp546-550.pdf), Tom LH. Li, Antoni B. Chan and Andy HW. Chun\n","* Missing: track segmentation and majority voting.\n","* Best seen: 17.6%"]},{"cell_type":"markdown","metadata":{"id":"3E429Deb_hKD"},"source":["## storing mfcc in npy"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"xile2oGqIxPd","executionInfo":{"status":"ok","timestamp":1733295646613,"user_tz":480,"elapsed":186,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":915748,"status":"ok","timestamp":1733269263826,"user":{"displayName":"Yushan Huang","userId":"08182600624189756141"},"user_tz":480},"id":"5bRdSOpnzQN4","outputId":"3bfed9b1-6ebc-4093-e4ef-97246277d2cb"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-54-8b3e48b90c72>:27: UserWarning: PySoundFile failed. Trying audioread instead.\n","  y, sr = librosa.load(input_file, sr=22050)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]},{"name":"stdout","output_type":"stream","text":["Error processing data/organized_small/train/Hip-Hop/068592.mp3: \n","saved 29 files\n","saved 25 files\n","saved 26 files\n","saved 34 files\n","saved 26 files\n","saved 35 files\n","saved 32 files\n","saved 34 files\n","saved 27 files\n","saved 23 files\n","saved 24 files\n","saved 32 files\n","saved 24 files\n","saved 33 files\n","saved 30 files\n","saved 32 files\n","saved 28 files\n","saved 24 files\n","saved 25 files\n","saved 33 files\n","saved 25 files\n","saved 34 files\n","saved 31 files\n","saved 33 files\n"]}],"source":["import os\n","import librosa\n","import numpy as np\n","\n","def preprocess_and_save_npy(input_root, output_root, metadata_path):\n","    shape = (13, 2582)\n","    metadata = pd.read_csv(metadata_path)\n","    for split in ['train', 'val', 'test']:\n","        input_dir = os.path.join(input_root, split)\n","        output_dir = os.path.join(output_root, split)\n","\n","        for genre_folder in os.listdir(input_dir):\n","            genre_input_dir = os.path.join(input_dir, genre_folder)\n","            genre_output_dir = os.path.join(output_dir, genre_folder)\n","\n","            if not os.path.isdir(genre_input_dir):\n","                continue\n","\n","            os.makedirs(genre_output_dir, exist_ok=True)\n","            for file in os.listdir(genre_input_dir):\n","                if not file.endswith('.mp3'):\n","                    continue\n","                input_file = os.path.join(genre_input_dir, file)\n","                output_file = os.path.join(genre_output_dir, f\"{os.path.splitext(file)[0]}.npy\")\n","                if not os.path.exists(output_file):  # Skip if already processed\n","                  try:\n","                    y, sr = librosa.load(input_file, sr=22050)\n","                    mfcc = librosa.feature.mfcc(y=y, sr=22050, n_mfcc=13, n_fft=512, hop_length=256)\n","                    padded_mfcc = np.zeros(shape, dtype=np.float32)\n","                    padded_mfcc[:, :min(mfcc.shape[1], shape[1])] = mfcc[:, :shape[1]]\n","                    padded_mfcc = padded_mfcc.T\n","                    np.save(output_file, padded_mfcc)\n","                    # print(f\"Saved: {output_file}\")\n","                  except Exception as e:\n","                    print(f\"Error processing {input_file}: {e}\")\n","            print(f\"saved {len(genre_output_dir)}\")\n","\n","# Example usage\n","input_dir = \"data/organized_small\"\n","output_dir = \"data/mfcc_small\"\n","metadata_path = \"data/organized_small/metadata.csv\"\n","preprocess_and_save_npy(input_dir, output_dir, metadata_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-xJC1JA2VyW"},"outputs":[],"source":["splits = ['train', 'val', 'test']\n","target_dir = \"data/mfcc_small\"\n","for split in splits:\n","    print(f\"\\n{split}:\")\n","    for genre in os.listdir(os.path.join(target_dir, split)):\n","      # print(genre)\n","      genre_path = os.path.join(target_dir, split, genre)\n","      n_files = len(os.listdir(genre_path))\n","      print(f\"  {genre}: {n_files} files\")"]},{"cell_type":"markdown","metadata":{"id":"Hrpsh-Rj_mjF"},"source":["## preprocessing"]},{"cell_type":"code","source":["# all_mfccs = []\n","# for file in \"data/mfcc_small\":\n","#     mfcc = np.load(file).astype(np.float32)\n","#     all_mfccs.append(mfcc)\n","# all_mfccs = np.array(all_mfccs)\n","# mean = np.mean(all_mfccs, axis=(0, 1))  # Mean along time and feature axis\n","# std = np.std(all_mfccs, axis=(0, 1))"],"metadata":{"id":"e1yI_OX51fVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"i4MFnR7h6noJ","executionInfo":{"status":"ok","timestamp":1733295626576,"user_tz":480,"elapsed":3343,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class AudioDataset(Dataset):\n","    def __init__(self, root_dir, split):\n","        \"\"\"\n","        Args:\n","            root_dir (str): Root directory for preprocessed data.\n","            split (str): One of 'train', 'val', or 'test'.\n","        \"\"\"\n","        self.root_dir = os.path.join(root_dir, split)\n","        self.file_paths = []\n","        self.labels = []\n","\n","        # Collect all file paths and their corresponding labels\n","        for label_idx, genre_folder in enumerate(sorted(os.listdir(self.root_dir))):\n","            # print(genre_folder)\n","            genre_dir = os.path.join(self.root_dir, genre_folder)\n","            if not os.path.isdir(genre_dir):\n","                continue\n","            files = [os.path.join(genre_dir, f) for f in os.listdir(genre_dir) if f.endswith('.npy')]\n","            self.file_paths.extend(files)\n","            self.labels.extend([label_idx] * len(files))\n","\n","\n","        # self.mean = torch.tensor([-280.04968, 114.869, -2.329334, 26.485447, 2.461681,\n","        #                           7.954758, -1.3554215, 3.4597769, -2.9572344, 2.5802717,\n","        #                           -2.7177243, 1.520156, -2.8376024]\n","        #                         )\n","        # self.std = torch.tensor([[117.78101, 49.52332, 36.08587, 23.085941, 19.979362, 17.462248,\n","        #                           15.67521, 14.535121, 13.653436, 13.152688, 12.6906, 12.248392,\n","        #                           11.782216]\n","        #                         ])\n","        # all_mfccs = []\n","        # for file in self.file_paths:\n","        #     mfcc = np.load(file).astype(np.float32)\n","        #     all_mfccs.append(mfcc)\n","        # all_mfccs = np.array(all_mfccs)\n","        # self.mean = np.mean(all_mfccs, axis=(0, 1))  # Mean along time and feature axis\n","        # self.std = np.std(all_mfccs, axis=(0, 1))\n","        # print(\"aoiejfaofjeoiwj\", self.mean, self.std)\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        file_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","\n","        # Load the .npy file\n","        mfcc = np.load(file_path).astype(np.float32)\n","        mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n","\n","        mfcc = torch.tensor(mfcc)\n","\n","        # mfcc = (mfcc- self.mean) / self.std\n","\n","        label = torch.tensor(label, dtype=torch.long)\n","        label = torch.nn.functional.one_hot(label, num_classes=8).float()\n","\n","        return mfcc, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elH5U7TADaWO"},"outputs":[],"source":["# class AudioDataset(Dataset):\n","#     def __init__(self, data_dir, split, labels_onehot, transform=None):\n","#         \"\"\"\n","#         Args:\n","#             data_dir: Root directory of organized dataset\n","#             split: 'train', 'val', or 'test'\n","#             metadata_path: Path to metadata.csv with labels\n","#             transform: Optional transform to be applied on features\n","#         \"\"\"\n","#         if split == \"training\":\n","#           self.data_dir = os.path.join(data_dir, 'train')\n","#         elif split == \"validation\":\n","#           self.data_dir = os.path.join(data_dir, 'val')\n","#         elif split == 'test':\n","#           self.data_dir = os.path.join(data_dir, split)\n","#         self.transform = transform\n","\n","#         # Load metadata\n","#         metadata = pd.read_csv(metadata_path)\n","#         # self.metadata = metadata[metadata['track_id'].isin(indices)]\n","#         self.metadata = metadata[metadata['split'] == split].copy()\n","#         # print(split)\n","#         print(self.metadata.shape)\n","#         self.metadata = self.metadata.drop('split', axis=1)\n","\n","#         # Separate track IDs and genre labels\n","#         self.track_ids = self.metadata['track_id'].values\n","#         self.labels = self.metadata.drop('track_id', axis=1).values  # Columns 1-8 are genre one-hot encodings\n","#         # print(indices.shape)\n","#         print(self.track_ids.shape)\n","#         # Get file paths\n","#         self.file_paths = []\n","#         for track_id in self.track_ids:\n","#           row = self.metadata[self.metadata['track_id'] == track_id]\n","#           genre_columns = list(self.metadata.columns[1:])  # Get genre column names\n","#           genre = genre_columns[row.drop('track_id', axis=1).values[0].argmax()]\n","\n","#           file_path = os.path.join(self.data_dir, genre, f\"{track_id:06d}.mp3\")\n","#           if os.path.exists(file_path):\n","#               self.file_paths.append(file_path)\n","#         print(len(self.file_paths))\n","#     def __len__(self):\n","#         return len(self.file_paths)\n","\n","#     def __getitem__(self, idx):\n","#         file_path = self.file_paths[idx]\n","#         label = torch.FloatTensor(self.labels[idx])\n","\n","#         # Load and process audio file\n","#         features = extract_mfcc_features(file_path)\n","\n","#         if self.transform:\n","#             features = self.transform(features)\n","\n","#         return features, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzcv95ypI-MV"},"outputs":[],"source":["# def extract_mfcc_features(file_path):\n","#   \"\"\"Extract MFCC features from audio file.\"\"\"\n","#   # Load audio file\n","#   shape = (13, 2582)\n","#   y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n","\n","#   # Extract MFCC features\n","#   mfcc = librosa.feature.mfcc(y=y, sr=22050, n_mfcc=13, n_fft=512, hop_length=256)\n","#   padded_mfcc = np.zeros(shape, dtype=np.float32)\n","#   padded_mfcc[:, :min(mfcc.shape[1], shape[1])] = mfcc[:, :shape[1]]\n","#   # print(padded_mfcc.shape)\n","#   padded_mfcc = padded_mfcc.T\n","\n","#   return padded_mfcc"]},{"cell_type":"markdown","metadata":{"id":"ls0HAXet_rSv"},"source":["## model + training"]},{"cell_type":"code","execution_count":89,"metadata":{"id":"mjPsTzWWJCyr","executionInfo":{"status":"ok","timestamp":1733302340482,"user_tz":480,"elapsed":320,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"outputs":[],"source":["import torch.nn.init as init\n","\n","\n","class RNNClassifier(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n","        super(RNNClassifier, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0,\n","            bidirectional=True\n","        )\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, num_classes),\n","            # nn.Sigmoid()\n","        )\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        # Initialize LSTM weights\n","        for name, param in self.lstm.named_parameters():\n","            if 'weight' in name:\n","                init.xavier_uniform_(param)\n","            elif 'bias' in name:\n","                init.zeros_(param)\n","\n","        # Initialize FC weights\n","        for name, param in self.fc.named_parameters():\n","            if 'weight' in name:\n","                init.xavier_uniform_(param)\n","            elif 'bias' in name:\n","                init.zeros_(param)\n","\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, sequence_length, input_size)\n","        lstm_out, _ = self.lstm(x)\n","\n","        # Use last time step output\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # Pass through fully connected layers\n","        output = self.fc(lstm_out)\n","        return output"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"YlY6NXBpJVAf","executionInfo":{"status":"ok","timestamp":1733300525361,"user_tz":480,"elapsed":176,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n","    \"\"\"Train the RNN model.\"\"\"\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        train_loss = 0\n","        for features, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","            features, labels = features.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(features)\n","            # print(outputs.dtype, labels.dtype)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        matched = [0]*8\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for features, labels in val_loader:\n","                features, labels = features.to(device), labels.to(device)\n","                outputs = model(features)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","                # print accuracy\n","                predicted = torch.argmax(outputs, dim=1)\n","                labels_one = torch.argmax(labels, dim=1)\n","\n","                for i in range(len(predicted)):\n","                  if predicted[i] == labels_one[i]:\n","                    matched[predicted[i]] += 1\n","                correct += (predicted == labels_one).sum().item()\n","                total += labels_one.numel()\n","\n","\n","        print(correct/total, [c/100 for c in matched])\n","\n","        # Print epoch statistics\n","        train_loss /= len(train_loader)\n","        val_loss /= len(val_loader)\n","        print(f'Epoch {epoch+1}/{num_epochs}:')\n","        print(f'Training Loss: {train_loss:.4f}', f'Validation Loss: {val_loss:.4f}')\n","        # print()\n","\n","        # Save best model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), 'best_model.pth')"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=0, alpha=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        if isinstance(alpha,(float,int,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n","        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n","        self.size_average = size_average\n","\n","    def forward(self, input, target):\n","        if input.dim()>2:\n","            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n","            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n","            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n","        target = target.long()\n","        target = target.argmax(dim=1)\n","\n","\n","\n","        logpt = F.log_softmax(input, dim=-1)\n","        logpt = logpt.gather(1,target.unsqueeze(1))\n","        logpt = logpt.view(-1)\n","        pt = Variable(logpt.data.exp())\n","\n","        if self.alpha is not None:\n","            if self.alpha.type()!=input.data.type():\n","                self.alpha = self.alpha.type_as(input.data)\n","            at = self.alpha.gather(0,target.data.view(-1))\n","            logpt = logpt * Variable(at)\n","\n","        loss = -1 * (1-pt)**self.gamma * logpt\n","        if self.size_average: return loss.mean()\n","        else: return loss.sum()"],"metadata":{"id":"zj2U8tLM59KN","executionInfo":{"status":"ok","timestamp":1733303430522,"user_tz":480,"elapsed":314,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"execution_count":108,"outputs":[]},{"cell_type":"code","execution_count":109,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1733303432718,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"},"user_tz":480},"id":"Ey7hglfJXBMC","outputId":"dea9e7f2-e9f9-4411-90c9-69a0d6101592"},"outputs":[{"output_type":"stream","name":"stdout","text":["2338\n","cuda\n"]}],"source":["# Constants\n","SAMPLE_RATE = 2000\n","N_MFCC = 13\n","HOP_LENGTH = 512\n","N_FFT = 2048\n","DURATION = 30  # seconds\n","# N_SEGMENTS = 10  # number of segments per song\n","\n","data_dir = \"data/organized_small\"\n","metadata_path = os.path.join(data_dir, \"metadata.csv\")\n","batch_size = 32\n","hidden_size = 16\n","num_layers = 3\n","num_classes = 8  # number of genres\n","learning_rate = 0.0001\n","num_epochs = 50\n","\n","# Create datasets and dataloaders\n","npy_root = \"data/mfcc_small\"\n","train_dataset = AudioDataset(npy_root, \"train\")\n","val_dataset = AudioDataset(npy_root, \"val\")\n","test_dataset = AudioDataset(npy_root, \"test\")\n","print(len(train_dataset))\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# for batch in train_loader:\n","#     print(batch[0].shape)\n","#     break\n","\n","# Initialize model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","input_size = N_MFCC  # MFCC + delta + delta-delta\n","model = RNNClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Set up training\n","# criterion = nn.CrossEntropyLoss()\n","criterion = FocalLoss(gamma=2, alpha=0.25, size_average=True)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","print(device)\n","# Train model\n","# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"wtSD2bzj5xS3","outputId":"6b2b0a0f-28d5-46e1-cbd5-0e7c210021df","executionInfo":{"status":"error","timestamp":1733303435211,"user_tz":480,"elapsed":622,"user":{"displayName":"Frank Wu","userId":"04656453812740581003"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/50:   0%|          | 0/74 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-110-61a2a3fbe913>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-72-d61eeb72aa7b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# print(outputs.dtype, labels.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-108-cc858f6db607>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mlogpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogpt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176,"status":"ok","timestamp":1733286108321,"user":{"displayName":"Frank Wu","userId":"05820992988364549060"},"user_tz":480},"id":"nvTwdM1CamdE","outputId":"4836306b-9a59-42fc-e113-ba4740d919a4"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-133-e87627cb2d87>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_model.pth'))\n"]}],"source":["# Load the best model\n","model.load_state_dict(torch.load('best_model.pth'))\n","model.to(device)\n","model.eval()\n","\n","def compute_accuracy(loader, model):\n","    correct = 0\n","    total = 0\n","    matched = [0]*8\n","    with torch.no_grad():\n","        for features, labels in loader:\n","            features, labels = features.to(device), labels.to(device)\n","            outputs = model(features)\n","            # print(labels)\n","            # predicted = (outputs > 0.5).float()  # Adjust for your task\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels_one = torch.argmax(labels, dim=1)\n","\n","            for i in range(len(predicted)):\n","              if predicted[i] == labels_one[i]:\n","                matched[predicted[i]] += 1\n","            correct += (predicted == labels_one).sum().item()\n","            total += labels_one.numel()\n","    print(\"correct classified: \", correct)\n","    print(\"total :\", total)\n","\n","    print([c/100 for c in matched])\n","    return correct / total"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":2889,"status":"error","timestamp":1733279941254,"user":{"displayName":"Frank Wu","userId":"05820992988364549060"},"user_tz":480},"id":"ZSzZVdf7sTx8","outputId":"7154c4bc-ffc4-4752-fe09-a3910373821d"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-7f6d9cd63bc1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final Training Accuracy: {train_accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final Validation Accuracy: {val_accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-49-e87627cb2d87>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_accuracy = compute_accuracy(train_loader, model)\n","val_accuracy = compute_accuracy(val_loader, model)\n","\n","print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n","print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5000,"status":"ok","timestamp":1733286116573,"user":{"displayName":"Frank Wu","userId":"05820992988364549060"},"user_tz":480},"id":"Mot9ISBFoZMd","outputId":"3a513112-ac61-4d66-e11f-7c770ca636d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["correct classified:  195\n","total : 800\n","[0.03, 0.02, 0.36, 0.5, 0.0, 0.57, 0.0, 0.47]\n","test accuracy; 0.2437\n"]}],"source":["test_accuracy = compute_accuracy(test_loader, model)\n","print(f\"test accuracy; {test_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a84ZFnJGJzoj"},"outputs":[],"source":["for raw_pred in raw_preds:\n","    pred_labels = np.argmax(raw_pred, axis=1)\n","    true_labels = np.argmax(test_labels, axis=1)\n","\n","    print(np.sum(pred_labels == true_labels)/800)\n","    correct = [0]*8\n","    for i in range(800):\n","        if pred_labels[i] == true_labels[i]:\n","            correct[true_labels[i]] +=1\n","\n","\n","    print([c/100 for c in correct])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["BCFDs5-AzOvf","vUUmvRT9_S9Q","3E429Deb_hKD"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}